---
title: "Bootstrap"
citeproc: true
bibliography: ../../static/bib/references.bib
csl: ../../static/bib/chicago-fullnote-bibliography-no-bib.csl
menu: 
  class:
    parent: Class
    weight: 14
type: docs
output:
  blogdown::html_page:
    toc: true
editor_options: 
  chunk_output_type: console
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#bootstrapping-concepts">Bootstrapping concepts</a>
<ul>
<li><a href="#bootstrapping-averages">Bootstrapping averages</a></li>
<li><a href="#bootstrapping-regression-coefficients">Bootstrapping regression coefficients</a></li>
</ul></li>
<li><a href="#standard-errors-and-confidence-intervals">Standard errors and confidence intervals</a></li>
<li><a href="#hypothesis-testing">Hypothesis testing</a>
<ul>
<li><a href="#hypothesis-testing-permutation">Hypothesis testing: permutation</a></li>
<li><a href="#alpha-levels-standards-of-evidence">Alpha-levels (standards of evidence)</a></li>
<li><a href="#hypothesis-testing-confidence-intervals">Hypothesis testing: confidence intervals</a></li>
</ul></li>
</ul>
</div>

<pre class="r"><code>library(tidyverse)
library(broom)
library(socviz)
library(infer)</code></pre>
<div id="bootstrapping-concepts" class="section level2">
<h2>Bootstrapping concepts</h2>
<p>Remember, our estimate is based on a sample from some population, and each sample is going to give us a different estimate. This means our estimates will <strong>vary</strong> from sample to sample. How can we quantify this <strong>variability</strong>?</p>
<p>One approach is via <strong>bootstrapping</strong>, where we:</p>
<ol style="list-style-type: decimal">
<li>Simulate many new datasets out of our original dataset</li>
<li>Estimate the thing we want to estimate in each of those <em>bootstrapped</em> samples</li>
<li>Look at the distribution of estimates across bootstrap samples</li>
</ol>
<p>That distribution of bootstrapped estimates gives us a sense for how much an estimate might vary from sample to sample.</p>
<div id="bootstrapping-averages" class="section level3">
<h3>Bootstrapping averages</h3>
<p>Let’s do this with the <code>infer</code> package, to estimate the number of kids the average American has.</p>
<pre class="r"><code>gss_sm %&gt;% 
  # specify the outcome variable
  specify(response = childs) %&gt;% 
  # generate the bootstrapped samples
  generate(reps = 1000, type = &quot;bootstrap&quot;)</code></pre>
<pre><code>## Response: childs (numeric)
## # A tibble: 2,859,000 x 2
## # Groups:   replicate [1,000]
##    replicate childs
##        &lt;int&gt;  &lt;dbl&gt;
##  1         1      2
##  2         1      0
##  3         1      4
##  4         1      2
##  5         1      3
##  6         1      2
##  7         1      4
##  8         1      4
##  9         1      3
## 10         1      1
## # … with 2,858,990 more rows</code></pre>
<p>Notice how each of the 1,000 bootstrapped samples has the same number of observations as the original dataset.</p>
<p>The above is equivalent to doing this 1,000 times:</p>
<pre class="r"><code>gss_sm %&gt;% 
  # subset down to just kids
  select(childs) %&gt;% 
  # sample with replacement, same size as original dataset
  sample_n(nrow(gss_sm), replace = TRUE)</code></pre>
<pre><code>## # A tibble: 2,867 x 1
##    childs
##     &lt;dbl&gt;
##  1      1
##  2      1
##  3      3
##  4      0
##  5      2
##  6      2
##  7      0
##  8      2
##  9      2
## 10      0
## # … with 2,857 more rows</code></pre>
<p>We can then calculate the average number of kids in each of those 1,000 bootstraps:</p>
<pre class="r"><code>boot_kids = gss_sm %&gt;% 
  # specify the outcome variable
  specify(response = childs) %&gt;% 
  # generate the bootstrapped samples
  generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% 
  # find the average # of kids in each bootstrap sample
  calculate(stat = &quot;mean&quot;)

boot_kids</code></pre>
<pre><code>## # A tibble: 1,000 x 2
##    replicate  stat
##        &lt;int&gt; &lt;dbl&gt;
##  1         1  1.87
##  2         2  1.87
##  3         3  1.86
##  4         4  1.87
##  5         5  1.82
##  6         6  1.86
##  7         7  1.85
##  8         8  1.87
##  9         9  1.92
## 10        10  1.87
## # … with 990 more rows</code></pre>
<p><code>stat</code> is the average number of kids in each bootstrap (<code>replicate</code>).</p>
<p>We can look at the distribution to get a sense for the variability in the estimates:</p>
<pre class="r"><code>ggplot(boot_kids, aes(x = stat)) + 
  geom_density(color = &quot;white&quot;, fill = &quot;coral&quot;, alpha = .7) + 
  theme_bw() + 
  labs(title = &quot;Average number of kids across bootstraps&quot;, 
       x = NULL, y = NULL) + 
  scale_x_continuous(limits = c(1, 3))</code></pre>
<p><img src="/class/bootstrap_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Notice how almost all bootstrapped estimates of the average number of kids are between 1.75 and 2, and the vast majority are in a narrower range than that.</p>
<p>We can also <strong>quantify</strong> this variation by taking the standard deviation of <code>stat</code>:</p>
<pre class="r"><code>boot_kids %&gt;% 
  summarise(se = sd(stat))</code></pre>
<pre><code>## # A tibble: 1 x 1
##       se
##    &lt;dbl&gt;
## 1 0.0311</code></pre>
<p>This is also known as the <strong>standard error</strong>.</p>
<div id="variability-gets-bigger-as-sample-size-gets-smaller" class="section level4">
<h4>Variability gets bigger as sample size gets smaller</h4>
<p>The process above gives us a sense of the variability in our estimate of the average number of kids in the US, based on our survey of <span class="math inline">\(\approx\)</span> 2,800 people. What if we had a much smaller survey? Say 100 people?</p>
<p>We can mimic that below by taking 100 random people from <code>gss_sm</code> and pretending that’s our full survey:</p>
<pre class="r"><code>boot_kids = gss_sm %&gt;% 
  # smaller survey of only 100 people
  sample_n(100) %&gt;% 
  # specify the outcome variable
  specify(response = childs) %&gt;% 
  # generate the bootstrapped samples
  generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% 
  # find the average # of kids in each bootstrap sample
  calculate(stat = &quot;mean&quot;)</code></pre>
<p>Notice how much wider this distribution is than the one above. The variability in our bootstrapped estimates is much higher!</p>
<pre class="r"><code>ggplot(boot_kids, aes(x = stat)) + 
  geom_density(color = &quot;white&quot;, fill = &quot;coral&quot;, alpha = .7) + 
  theme_bw() + 
  labs(title = &quot;Average number of kids across bootstraps&quot;, 
       x = NULL, y = NULL) + 
  scale_x_continuous(limits = c(1, 3))</code></pre>
<p><img src="/class/bootstrap_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>You can quantify this too; notice how much bigger the standard error is of this much smaller survey:</p>
<pre class="r"><code>boot_kids %&gt;% 
  summarise(se = sd(stat))</code></pre>
<pre><code>## # A tibble: 1 x 1
##      se
##   &lt;dbl&gt;
## 1 0.190</code></pre>
</div>
</div>
<div id="bootstrapping-regression-coefficients" class="section level3">
<h3>Bootstrapping regression coefficients</h3>
<p>We can use bootstrapping to approximate the variability in lots of things we might want to estimaet, including coefficients from regression.</p>
<p>Say we wanted to look at the effect of age on whether or not someone has children:</p>
<pre class="r"><code>lm(childs ~ age, data = gss_sm) %&gt;% 
  tidy()</code></pre>
<pre><code>## # A tibble: 2 x 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   0.153    0.0858       1.79 7.40e- 2
## 2 age           0.0345   0.00164     21.0  1.83e-91</code></pre>
<p>Remember this estimate comes from a <em>sample</em>. Other samples will give us different estimates. We can get a sense for how much coefficients will vary across samples with bootstrapping:</p>
<pre class="r"><code>boot_age = gss_sm %&gt;% 
  # specify is now Y ~ X
  specify(childs ~ age) %&gt;% 
  generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% 
  # &quot;slope&quot; is another word for coefficient
  calculate(stat = &quot;slope&quot;)</code></pre>
<p>Notice how the code looks a bit different: we put <code>childs ~ age</code> in <code>specify()</code> and we gotta tell R to calculate the “slope”, or coefficient, from <code>lm(childs ~ age)</code>.</p>
<p>We can plot:</p>
<pre class="r"><code>ggplot(boot_age, aes(x = stat)) + 
  geom_density(color = &quot;white&quot;, fill = &quot;coral&quot;, alpha = .7) + 
  theme_bw() + 
  labs(title = &quot;Estimate of relationship between age and number of kids&quot;,
       subtitle = &quot;Coefficient estimates across bootstrapped samples.&quot;,
       x = NULL, y = NULL)</code></pre>
<p><img src="/class/bootstrap_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Notice how coefficient estimates vary a lot, from .03 to almost .04.</p>
</div>
</div>
<div id="standard-errors-and-confidence-intervals" class="section level2">
<h2>Standard errors and confidence intervals</h2>
<p>The distributions we get from bootstrapping give us a sense for the <strong>variability</strong> in our sample estimates. We can quantify that variability in two ways:</p>
<p>One is through the standard error (the standard deviation of the bootstrapped sample estimates):</p>
<pre class="r"><code>boot_age %&gt;% 
  summarise(se = sd(stat))</code></pre>
<pre><code>## # A tibble: 1 x 1
##        se
##     &lt;dbl&gt;
## 1 0.00167</code></pre>
<p>The other is the confidence interval, which provides our “best guess” of the thing we are trying to estimate:</p>
<pre class="r"><code>boot_age %&gt;% get_confidence_interval(level = .95)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   lower_ci upper_ci
##      &lt;dbl&gt;    &lt;dbl&gt;
## 1   0.0314   0.0377</code></pre>
<p>The standard is 95%: so the two numbers give us the upper and lower bound of the middle 95% of the bootstrapped distribution.</p>
<p>Notice that as the confidence increases, the bounds get larger:</p>
<pre class="r"><code>boot_age %&gt;% get_confidence_interval(level = .99)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   lower_ci upper_ci
##      &lt;dbl&gt;    &lt;dbl&gt;
## 1   0.0305   0.0387</code></pre>
<p>As the confidence decreases, the bounds get smaller:</p>
<pre class="r"><code>boot_age %&gt;% get_confidence_interval(level = .5)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   lower_ci upper_ci
##      &lt;dbl&gt;    &lt;dbl&gt;
## 1   0.0334   0.0356</code></pre>
</div>
<div id="hypothesis-testing" class="section level2">
<h2>Hypothesis testing</h2>
<p>Remember, hypothesis testing is about how we decide between two competing hypotheses – or theories about the relationship between two variables – using data.</p>
<p>Take the example from class, on whether <a href="https://www.theatlantic.com/health/archive/2013/05/study-mens-biceps-predict-their-political-ideologies/275942/">bicep size predicts conservative ideology</a>. The two competing hypotheses are:</p>
<ol style="list-style-type: decimal">
<li>Null hypothesis: there is no relationship between biceps and conservative ideology</li>
<li>Alternative hypothesis: there is a positive relationship between biceps and conservative ideology</li>
</ol>
<p>We make up fake data below:</p>
<pre class="r"><code>set.seed(23424)
# fake bicep data
fake = tibble(person = 1:100, 
              bicep = rnorm(100), 
              conservative = runif(100)*100 + 2*bicep)</code></pre>
<p>We estimate the relationship between bicep and ideology:</p>
<pre class="r"><code># what is the effect?
lm(conservative ~ bicep, data = fake) %&gt;% summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = conservative ~ bicep, data = fake)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -45.688 -24.505  -5.035  28.053  54.741 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   46.839      2.892  16.197   &lt;2e-16 ***
## bicep          3.996      2.754   1.451     0.15    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 28.92 on 98 degrees of freedom
## Multiple R-squared:  0.02103,    Adjusted R-squared:  0.01104 
## F-statistic: 2.105 on 1 and 98 DF,  p-value: 0.15</code></pre>
<p>Remember, this estimate is based on one sample. How much might estimates vary? We can use bootstrapping to get a sense:</p>
<pre class="r"><code>boot_bicep = fake %&gt;% 
  specify(conservative ~ bicep) %&gt;% 
  generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% 
  calculate(stat = &quot;slope&quot;)


ggplot(boot_bicep, aes(x = stat)) + 
  geom_density(color = &quot;white&quot;, fill = &quot;coral&quot;, alpha = .7) + 
  theme_bw() + 
  labs(title = &quot;Estimate of relationship between bicep size and ideology score&quot;,
       subtitle = &quot;Coefficient estimates across bootstrapped samples.&quot;,
       x = NULL, y = NULL)</code></pre>
<p><img src="/class/bootstrap_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Notice how much the coefficient can vary: in some cases as large as 10 or more, in others 0, and in some cases even negative.</p>
<p>How do we decide, based on this distribution, whether there is in fact that a stable relationship between biceps and ideology?</p>
<div id="hypothesis-testing-permutation" class="section level3">
<h3>Hypothesis testing: permutation</h3>
<p>One way to decide is to simulate what the world might look like under the null hypothesis – a world where biceps don’t affect ideology.</p>
<p>We can do this by randomly “shuffling” or <em>permuting</em> the values in our bicep variable. Why? This mimics the idea that if bicep size and ideology are unrelated, you could completely alter one variable without affecting the other. So we:</p>
<ol style="list-style-type: decimal">
<li>“shuffle” or permute the bicep variable</li>
<li>estimate <code>lm(conservative ~ bicep)</code>, store coefficient on bicep</li>
<li>repeat N times</li>
</ol>
<pre class="r"><code># permutation based hypothesis testing
null_biceps = fake %&gt;% 
  # specify is now Y ~ X
  specify(conservative ~ bicep) %&gt;% 
  # define null hypothesis: &quot;independence&quot;
  hypothesize(null = &quot;independence&quot;) %&gt;% 
  # now we do &quot;permute&quot; instead of bootstrap
  generate(reps = 1000, type = &quot;permute&quot;) %&gt;% 
  # &quot;slope&quot; is another word for coefficient
  calculate(stat = &quot;slope&quot;)

null_biceps</code></pre>
<pre><code>## # A tibble: 1,000 x 2
##    replicate   stat
##        &lt;int&gt;  &lt;dbl&gt;
##  1         1  3.60 
##  2         2  3.28 
##  3         3  1.73 
##  4         4  0.390
##  5         5  2.15 
##  6         6  5.20 
##  7         7 -5.17 
##  8         8 -4.43 
##  9         9 -2.34 
## 10        10  1.15 
## # … with 990 more rows</code></pre>
<p>Each row here is the estimated coefficient of bicep from a permuted sample – a sample where the values of bicep have been shuffled. Look at the distribution:</p>
<pre class="r"><code>ggplot(null_biceps, aes(x = stat)) + geom_histogram(color = &quot;white&quot;, 
                                                     fill = &quot;coral&quot;) + 
  theme_bw() + 
  labs(title = &quot;The Simulated Null Distribution&quot;, 
       subtitle = &quot;What we would observe in a world where biceps and ideology are unrelated.&quot;, 
       x = &quot;Permuted coefficient estimates from lm(affairs ~ children)&quot;, 
       y = NULL)</code></pre>
<p><img src="/class/bootstrap_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>This is our simulation of what we might observe under the <em>null hypothesis</em> – that there is no relationship between biceps and ideology. Two key takeaways here:</p>
<ol style="list-style-type: decimal">
<li><p>As we would expect, most of our simulated coefficient estimates are close to zero. This makes sense! The null hypothesis is precisely that there is no relationship between the two (e.g., that the coefficient is not greater than zero!).</p></li>
<li><p>Even in a world where the null is true, you can still get pretty large coefficient estimates by chance, even as large as -10 and +10.</p></li>
</ol>
<p>Point (2) is really important: totally by chance, two variables that are unrelated can still have strong positive or negative correlations. It is <em>unlikely that this happens, but it’s still possible</em>.</p>
<p>With a simulation of what the world would like under the null hypothesis, we can turn back to our original estimate:</p>
<pre class="r"><code>lm(conservative ~ bicep, data = fake) %&gt;% summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = conservative ~ bicep, data = fake)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -45.688 -24.505  -5.035  28.053  54.741 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   46.839      2.892  16.197   &lt;2e-16 ***
## bicep          3.996      2.754   1.451     0.15    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 28.92 on 98 degrees of freedom
## Multiple R-squared:  0.02103,    Adjusted R-squared:  0.01104 
## F-statistic: 2.105 on 1 and 98 DF,  p-value: 0.15</code></pre>
<p>How likely is it that our estimate would have occurred by chance?</p>
<pre class="r"><code>ggplot(null_biceps, aes(x = stat)) + geom_histogram(color = &quot;white&quot;, 
                                                     fill = &quot;coral&quot;) + 
  theme_bw() + 
  labs(title = &quot;The Simulated Null Distribution&quot;, 
       subtitle = &quot;What we would observe in a world where biceps and ideology are unrelated.&quot;, 
       x = &quot;Permuted coefficient estimates from lm(affairs ~ children)&quot;, 
       y = NULL) + 
  geom_vline(xintercept = 3.996, lty = 2, color = &quot;black&quot;, size = 2)</code></pre>
<p><img src="/class/bootstrap_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Our estimate is a fair bit larger than what you would expect to observe under the null hypothesis. We can calculate exactly how much larger using the <strong>p-value</strong>. We set <code>direction = right</code> because we want to know how much <em>larger</em> the estimate is than expected.</p>
<pre class="r"><code>get_p_value(null_biceps, obs_stat = 3.996, direction = &quot;right&quot;)</code></pre>
<pre><code>## # A tibble: 1 x 1
##   p_value
##     &lt;dbl&gt;
## 1   0.084</code></pre>
<p>The <strong>p-value</strong> is .084, meaning that only 8.4% of the estimates we got by chance are as large or larger than our original result. In other words, our estimate is larger than 84% of the results we could observe by chance.</p>
<p>We can calculate this “by hand” too, like so:</p>
<pre class="r"><code>null_biceps %&gt;% 
  mutate(bigger = ifelse(stat &gt; 3.996, &quot;yes&quot;, &quot;no&quot;)) %&gt;% 
  group_by(bigger) %&gt;% 
  tally() %&gt;% 
  mutate(percent = n/sum(n))</code></pre>
<pre><code>## # A tibble: 2 x 3
##   bigger     n percent
##   &lt;chr&gt;  &lt;int&gt;   &lt;dbl&gt;
## 1 no       916   0.916
## 2 yes       84   0.084</code></pre>
<p>84% larger than we might observe by chance – sounds like our result is unusually large, and unlikely to be the result of random chance. But is it unlikely enough for us to be confident?</p>
</div>
<div id="alpha-levels-standards-of-evidence" class="section level3">
<h3>Alpha-levels (standards of evidence)</h3>
<p>Remember the standard and totally arbitrary <em>alpha-level</em> accepted in the scientific community is .05. This means that p-values lower than .05 are considered too small to be the result of random chance, while p-values higher than that are considered too large to feel confident in. Arbitrary? Yes.</p>
<p>Since our p-value of .084 is larger than the .05 standard (the alpha-level), we would <em>fail to reject the null hypothesis</em>. In other words, the result is too plausible in a world where null hypothesis is true for us to reject it!</p>
</div>
<div id="hypothesis-testing-confidence-intervals" class="section level3">
<h3>Hypothesis testing: confidence intervals</h3>
<p>A different, and much more straightforward way, to do hypothesis testing is to bootstrap confidence intervals and see if they exclude 0. Let’s do the bootstrap.</p>
<pre class="r"><code>boot_bicep = fake %&gt;% 
  specify(conservative ~ bicep) %&gt;% 
  generate(reps = 1000, type = &quot;boot&quot;) %&gt;% 
  calculate(stat = &quot;slope&quot;)</code></pre>
<p>Here is the distribution of bootstrapped estimates from our data:</p>
<pre class="r"><code>ggplot(boot_bicep, aes(x = stat)) + geom_histogram(color = &quot;white&quot;, 
                                                     fill = &quot;coral&quot;) + 
  theme_bw() + 
  labs(title = &quot;Distribution of bootstrapped estimates&quot;, 
       subtitle = &quot;How the relationship between biceps and ideology might vary across samples.&quot;, 
       x = &quot;Bootstrapped coefficient estimates from lm(conservative ~ bicep)&quot;, 
       y = NULL)</code></pre>
<p><img src="/class/bootstrap_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>Here’s the 95% confidence interval:</p>
<pre class="r"><code>get_confidence_interval(boot_bicep, level = .95)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   lower_ci upper_ci
##      &lt;dbl&gt;    &lt;dbl&gt;
## 1    -1.58     9.23</code></pre>
<p>Notice this interval <em>includes</em> 0. This means that the range of values that are our “best guess” for the effect of biceps on ideology <strong>include</strong> the possibility that there is no effect (or that the effect is negative). Following the standard of using .05 as the threshold, we would <em>fail to reject the null hypothesis of no effect of biceps on ideology</em>.</p>
</div>
</div>
